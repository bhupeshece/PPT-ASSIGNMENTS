{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f1dd9e",
   "metadata": {},
   "source": [
    "# Naive Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388f3d8",
   "metadata": {},
   "source": [
    "#q1\n",
    " Naive Approach, often referred to as the Naive Bayes classifier, is a simple yet effective algorithm used in machine learning for classification tasks. It is based on the concept of Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77aee88",
   "metadata": {},
   "source": [
    "#q2\n",
    "\n",
    "This assumption implies that the presence or absence of a particular feature in a class is independent of the presence or absence of other features, given the class label\n",
    "which depends on\n",
    "\n",
    "Conditional Independence\n",
    "\n",
    "Simplifying Assumption\n",
    "\n",
    "Impact on Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0836e",
   "metadata": {},
   "source": [
    "#q3\n",
    " when training a naive Bayes classifier. we can choose to either omit records with any missing values or omit only the missing attributes\n",
    "    this way the Naive Approach handle missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249030ba",
   "metadata": {},
   "source": [
    "#q4\n",
    "\n",
    " advantage is that it is inexpensive to develop, store data, and operate. The disadvantage is that it does not consider any possible causal relationships that underly the forecasted variable. This model adds the latest observed absolute period -to-period change to the most recent observed level of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22b7d3",
   "metadata": {},
   "source": [
    "#q5\n",
    "\n",
    "Naive Bayes classifier is not typically used for regression problems where the goal is to predict continuous numerical values. It is primarily designed for classification tasks, which involve predicting discrete class labels. For regression problems, other algorithms such as linear regression, decision trees, random forests, support vector regression (SVR), and neural networks are more suitable options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83fce1",
   "metadata": {},
   "source": [
    "#q6\n",
    "\n",
    "We can use naïve bayes classifier for categorical variables using one-hot encoding. If we have n categories then we create n-1 dummy variables or features and add to our data. If the particular category is associated with a row then we assign it as 1 otherwise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c416c",
   "metadata": {},
   "source": [
    "#q7\n",
    "\n",
    "Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm\n",
    "\n",
    "It is introduced to solve the problem of zero probability i.e. when a query point contains a new observation, which is not yet seen in training data while calculating probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ecbe6",
   "metadata": {},
   "source": [
    "#q8\n",
    "\n",
    "choice of the probability threshold depends on the specific requirements and trade-offs of your application. The threshold is used to make a decision regarding which class a given instance belongs to based on the calculated probabilities\n",
    "few approaches are:\n",
    "    \n",
    "    Default Threshold\n",
    "    \n",
    "    Adjusting Threshold\n",
    "    \n",
    "    Receiver Operating Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432a253",
   "metadata": {},
   "source": [
    "#q9\n",
    "\n",
    "Naive Approach can be applied is in pattern searching. The Naive Algorithm is the simplest method which uses brute force approach. It compares first character of pattern with searchable text. If match is found, pointers in both strings are advanced. If match not found, pointer of text is incremented and pointer of pattern is reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc12822",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b0df3",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "A. KNN classifier is a machine learning algorithm used for classification and regression problems. It works by finding the K nearest points in the training dataset and uses their class to predict the class or value of a new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba57c4e",
   "metadata": {},
   "source": [
    "#Q11\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bf6de",
   "metadata": {},
   "source": [
    "#Q12\n",
    "\n",
    "value of K in KNN is a hyperparameter that controls the number of neighbors that are considered when making a prediction.\n",
    "\n",
    "There are different methods to choose the optimal value of K such as Grid Search, Random Search, Cross-Validation etc. Grid Search is an exhaustive search over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3ef12",
   "metadata": {},
   "source": [
    "#Q13\n",
    "\n",
    "Advantages=\n",
    "\n",
    "Quick calculation time.\n",
    "\n",
    "Simple algorithm – to interpret.\n",
    "\n",
    "Versatile – useful for regression and classification.\n",
    "\n",
    "High accuracy – you do not need to compare with better-supervised learning models.\n",
    "\n",
    "Disadvantages=\n",
    "\n",
    "\n",
    "Doesn't work well with a large dataset: Since KNN is a distance-based algorithm, the cost of calculating distance between a new point and each existing point is very high which in turn degrades the performance of the algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f866f71",
   "metadata": {},
   "source": [
    "#q14\n",
    "\n",
    "The distance metric determines how similarity or dissimilarity is calculated between data instances in KNN. Different distance metrics emphasize different aspects of the data, which can lead to variations in the algorithm's performance\n",
    "hoice of distance metric in KNN affects how similarity is measured between instances, and selecting an appropriate distance metric is crucial for achieving accurate and effective results in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa54910",
   "metadata": {},
   "source": [
    "#q15\n",
    "Yes, KNN can handle imbalanced datasets. One way to handle imbalanced datasets is to use undersampling. In supervised learning, the imbalanced number of instances among the classes in a dataset can make the algorithms classify one instance from the minority class as one from the majority class. With the aim to solve this problem, the KNN algorithm provides a basis to other balancing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3fcbf",
   "metadata": {},
   "source": [
    "#16\n",
    "Handling categorical features in the k-nearest neighbors (KNN) algorithm requires encoding the categorical variables into a numerical format that can be used in distance calculations. \n",
    "\n",
    "some approaches are:\n",
    "\n",
    "One-Hot Encoding\n",
    "\n",
    "Label Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211591d",
   "metadata": {},
   "source": [
    "#q17\n",
    "\n",
    "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5164505",
   "metadata": {},
   "source": [
    "#q18\n",
    "\n",
    "KNN can be applied in various scenarios such as image recognition, recommender systems, and anomaly detection. For example, in image recognition, KNN can be used to classify images based on their similarity to other images1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379c076",
   "metadata": {},
   "source": [
    "# Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7813c6c",
   "metadata": {},
   "source": [
    "#q19\n",
    "\n",
    "Clustering is the act of organizing similar objects into groups within a machine learning algorithm. Assigning related objects into clusters is beneficial for AI models. Clustering has many uses in data science, like image processing, knowledge discovery in data, unsupervised learning, and various other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d458a94",
   "metadata": {},
   "source": [
    "#q20\n",
    "\n",
    "Hierarchical Clustering:=\n",
    "\n",
    "Hierarchical clustering is a bottom-up or top-down approach that builds a hierarchy of clusters.\n",
    "\n",
    "It does not require a pre-specified number of clusters.\n",
    "\n",
    "It starts with each data point as an individual cluster and then iteratively merges or splits clusters based on the similarity between them.\n",
    "\n",
    "\n",
    "K-means Clustering:=\n",
    "\n",
    "K-means clustering is an iterative algorithm that partitions the data into k distinct clusters.\n",
    "\n",
    "\n",
    "It requires specifying the number of clusters (k) in advance.\n",
    "\n",
    "It starts by randomly selecting k cluster centroids and then assigns data points to the nearest centroid based on distance (usually Euclidean distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89290ccd",
   "metadata": {},
   "source": [
    "#q21\n",
    "\n",
    "The optimal number of clusters in k-means clustering can be determined using the elbow method. The elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3838f",
   "metadata": {},
   "source": [
    "#q22\n",
    "\n",
    "Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668dab75",
   "metadata": {},
   "source": [
    "#q23\n",
    "\n",
    "Handling categorical features in clustering requires transforming them into a numerical format that can be used effectively in distance-based clustering algorithms. Here are a few common approaches:\n",
    "    common approaches are=\n",
    "    \n",
    "    One-Hot Encoding\n",
    "    \n",
    "    Frequency Encoding\n",
    "    \n",
    "    Ordinal Encoding\n",
    "    \n",
    "    Feature Embedding: \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7eef9",
   "metadata": {},
   "source": [
    "#q24\n",
    "\n",
    "main advantages of hierarchical clustering is that it provides detailed information about which observations are most similar to each other. This level of detail is not provided by many other algorithms, which generally just return the ID of the cluster a given observation belongs to.\n",
    "\n",
    "\n",
    "Disadvantage\n",
    "\n",
    "\n",
    " Centralization of Power.\n",
    "    \n",
    "it rarely provides the best solution, it involves lots of arbitrary decisions, it does not work with missing data, it\n",
    "works poorly with mixed data types, it does not work well on very large data sets, and its main output, the dendrogram, is commonly misinterpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb36576",
   "metadata": {},
   "source": [
    "#q25\n",
    "\n",
    "The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1 where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "The silhouette score is calculated for each sample in the dataset as follows:\n",
    "\n",
    "Calculate the average distance between each sample and all other points in the same cluster.\n",
    "Calculate the average distance between each sample and all other points in the next nearest cluster.\n",
    "Calculate the silhouette score for each sample as (b - a) / max(a, b), where a is the average distance between the sample and all other points in the same cluster, and b is the average distance between the sample and all other points in the next nearest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a6de4",
   "metadata": {},
   "source": [
    "#q26\n",
    "\n",
    "Clustering can be applied in various scenarios such as customer segmentation, anomaly detection, and image segmentation. For example, in customer segmentation, clustering can be used to group customers based on their purchasing behavior or demographics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a747ba",
   "metadata": {},
   "source": [
    "#q27\n",
    "\n",
    "Anomaly detection is identifying data points in data that don't fit the normal patterns. It can be useful to solve many problems including fraud detection, medical diagnosis, etc. Machine learning methods allow to automate anomaly detection and make it more effective, especially when large datasets are involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469acd8",
   "metadata": {},
   "source": [
    "#q28\n",
    "\n",
    "Supervised anomaly detection is a type of anomaly detection that involves training a model on labeled data to identify anomalies. The model is trained to recognize patterns in the data that are associated with normal behavior and then uses these patterns to identify anomalies in new data.\n",
    "\n",
    "Unsupervised anomaly detection, on the other hand, does not require labeled data. Instead, it involves identifying patterns in the data that are different from the majority of the data. This can be done using clustering algorithms or density-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3e8ab",
   "metadata": {},
   "source": [
    "#q29\n",
    "\n",
    "Outliers or anomaly detection can be detected using the Box-Whisker method or by DBSCAN.\n",
    "\n",
    "Euclidean distance method is used with the items not correlated.\n",
    "\n",
    "Mahalanobis method is used with Multivariate outliers.\n",
    "\n",
    "All the values or points are not outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069b7c5",
   "metadata": {},
   "source": [
    "#q30\n",
    "\n",
    "One-Class SVM (Support Vector Machine) algorithm is a popular method used for anomaly detection, where the goal is to identify observations that deviate significantly from the normal behavior of a dataset.\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "The One-Class SVM algorithm is trained on a dataset that contains only normal or non-anomalous instances.\n",
    "The algorithm learns a decision boundary that encloses the majority of the normal instances in a feature space.\n",
    "\n",
    "Testing Phase:\n",
    "\n",
    "During the testing phase, the trained One-Class SVM model is used to classify new instances as normal or anomalous.\n",
    "The algorithm maps the new instances into the feature space and checks their position relative to the learned decision boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ed504",
   "metadata": {},
   "source": [
    "#q31\n",
    "\n",
    "The appropriate threshold for anomaly detection depends on the specific application and the desired trade-off between false positives and false negatives.\n",
    "\n",
    "A high threshold will result in fewer false positives but may also result in more false negatives. A low threshold will result in more false positives but may also result in fewer false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814bc4a4",
   "metadata": {},
   "source": [
    "#q32\n",
    "\n",
    "Imbalanced datasets can be handled in anomaly detection by using techniques such as oversampling, undersampling, or generating synthetic samples.\n",
    "\n",
    "Oversampling involves increasing the number of samples in the minority class by duplicating existing samples or generating new ones. Undersampling involves reducing the number of samples in the majority class by randomly removing samples. Synthetic sample generation involves creating new samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN (Adaptive Synthetic Sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe25014",
   "metadata": {},
   "source": [
    "#q33\n",
    "\n",
    "In Credit Card Fraud Detection:\n",
    "A credit card company wants to protect its customers from fraudulent transactions. They collect data on credit card transactions, including transaction amounts, merchant information, time of transaction, and customer demographics.\n",
    "\n",
    "Anomaly detection can be used to identify unusual or suspicious transactions that may indicate fraudulent activity. By analyzing patterns in the data, the credit card company can detect anomalies and take appropriate action to prevent fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64feb6",
   "metadata": {},
   "source": [
    "# Dimension Reduction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557686bf",
   "metadata": {},
   "source": [
    "#34\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaafcba",
   "metadata": {},
   "source": [
    "#35\n",
    "\n",
    "Feature selection refers to the process of selecting a subset of relevant features from the original feature set. It aims to identify and keep only the most informative and discriminative features that have a strong relationship with the target variable. The goal is to reduce the dimensionality of the data by discarding irrelevant or redundant features.\n",
    "\n",
    "Feature extraction involves transforming the original features into a new set of derived features. It aims to capture the essential information contained in the original features and represent it in a more compact and informative way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfa1cd",
   "metadata": {},
   "source": [
    "#q36\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique used for dimensionality reduction. It works by finding the directions of maximum variance in high-dimensional data and projecting the data onto a lower-dimensional space while retaining as much of the original variance as possible.\n",
    "\n",
    "PCA involves the following steps:\n",
    "\n",
    "Standardize the data by subtracting the mean and dividing by the standard deviation\n",
    ".\n",
    "Compute the covariance matrix of the standardized data.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec4d35",
   "metadata": {},
   "source": [
    "#q37\n",
    "\n",
    "Choosing the number of components (also known as the number of principal components) in Principal Component Analysis (PCA) involves finding a balance between reducing the dimensionality of the data and retaining enough information to accurately represent the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305c034",
   "metadata": {},
   "source": [
    "#q38\n",
    "\n",
    "Dimensionality Reducion Techniques=\n",
    "\n",
    "Feature selection. \n",
    "\n",
    "Feature extraction. \n",
    "\n",
    "Principal Component Analysis (PCA) \n",
    "\n",
    "Non-negative matrix factorization (NMF)\n",
    "\n",
    "Linear discriminant analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2dd17",
   "metadata": {},
   "source": [
    "#q39\n",
    "\n",
    "Dimension reduction can be applied in various scenarios such as image processing, text mining, and bioinformatics.\n",
    "\n",
    "For example, in image processing, dimension reduction can be used to reduce the number of pixels in an image while retaining as much of the original information as possible. This can be useful for compressing images or reducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ca0c3",
   "metadata": {},
   "source": [
    "# Feature Selection:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdbe99",
   "metadata": {},
   "source": [
    "#q40\n",
    "\n",
    "Feature selection is the process of isolating the most consistent, non-redundant, and relevant features to use in model construction. Methodically reducing the size of datasets is important as the size and variety of datasets continue to grow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67db327",
   "metadata": {},
   "source": [
    "#q41\n",
    "\n",
    "Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd6adbc",
   "metadata": {},
   "source": [
    "#q42\n",
    "\n",
    "Correlation-based feature selection is a technique used to select relevant features from a dataset by measuring the correlation between features and the target variable. It helps identify features that have a strong relationship with the target, while discarding features that are weakly correlated or unrelated. Here's how correlation-based feature selection works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f60b6",
   "metadata": {},
   "source": [
    "#q43\n",
    "\n",
    "Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated with each other. This can lead to unstable estimates of the regression coefficients and reduced predictive power of the model\n",
    "\n",
    "methods for handling multicollinearity in feature selection:\n",
    "\n",
    "Remove one of the correlated variables: This is the simplest method and involves removing one of the variables that is highly correlated with another variable.\n",
    "\n",
    "Combine the correlated variables: This involves creating a new variable that is a linear combination of the correlated variables.\n",
    "\n",
    "Use regularization techniques: Regularization techniques such as Lasso or Ridge regression can be used to reduce the impact of multicollinearity on the regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71bd41",
   "metadata": {},
   "source": [
    "#q44\n",
    "\n",
    "Feature Selection Methods in ML=\n",
    "\n",
    "Chi-square Test. The Chi-square test is used for categorical features in a dataset.\n",
    "\n",
    "Fisher's Score. \n",
    "\n",
    "Correlation Coefficient.\n",
    "\n",
    "Dispersion Ratio.\n",
    "\n",
    "Backward Feature Elimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270827c",
   "metadata": {},
   "source": [
    "#q45\n",
    "\n",
    "Feature selection can be applied in various scenarios such as image processing, text mining, and bioinformatics.\n",
    "\n",
    "For example, in image processing, feature selection can be used to identify the most important features (such as color, texture, or shape) that are relevant for a particular task such as object recognition or image segmentation.\n",
    "\n",
    "In text mining, feature selection can be used to identify the most important words or phrases that are relevant for a particular task such as sentiment analysis or topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d3f8c",
   "metadata": {},
   "source": [
    "# Data Drift Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b213b76",
   "metadata": {},
   "source": [
    "#q46\n",
    "\n",
    "For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350aa93a",
   "metadata": {},
   "source": [
    "#q47\n",
    "\n",
    "to ensure that your machine learning models remain accurate and reliable over time.\n",
    "By understanding the causes and effects of drift, and implementing effective drift monitoring practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa075a43",
   "metadata": {},
   "source": [
    "#q48\n",
    "\n",
    "Data drift refers to the changing distribution of the data to which the model is applied. Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift can lead to a decline in the performance of a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa5ddb",
   "metadata": {},
   "source": [
    "#q49\n",
    "\n",
    "Data drift occurs when the statistical properties of a dataset change over time. This can lead to reduced performance of machine learning models that were trained on the original dataset. There are several techniques for detecting data drift:\n",
    "\n",
    "Monitoring performance metrics: This involves monitoring performance metrics such as accuracy, precision, recall, or F1 score over time. A drop in performance may indicate that data drift has occurred.\n",
    "\n",
    "Statistical tests: This involves using statistical tests such as the Kolmogorov-Smirnov test or the Mann-Whitney U test to compare the statistical properties of two datasets.\n",
    "\n",
    "Drift detection algorithms: There are several drift detection algorithms such as ADWIN (Adaptive Windowing), DDM (Drift Detection Method), and EDDM (Early Drift Detection Method) that can be used to detect data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf393b",
   "metadata": {},
   "source": [
    "#q50\n",
    "\n",
    "few techniques are=\n",
    "\n",
    "Continuous Monitoring: Implement a system for continuous monitoring of the input data and model performance. Regularly track and compare the distribution of the incoming data with the distribution of the training data. Detecting and identifying data drift at an early stage is key to taking appropriate actions.\n",
    "\n",
    "Retraining the Model: When significant data drift is detected, consider retraining the model using the updated or recent data. Accumulate new data over time and periodically update the model to capture the evolving patterns and dynamics of the data. This ensures that the model remains up to date and can adapt to the changing environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f780d9d",
   "metadata": {},
   "source": [
    "# Data Leakage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39595e65",
   "metadata": {},
   "source": [
    "#q51\n",
    "\n",
    "Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting\n",
    "\n",
    "eg-\n",
    "The most obvious and easy to understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51588332",
   "metadata": {},
   "source": [
    "#q52\n",
    "\n",
    "Data leakage is a significant concern in machine learning and data analysis because it can lead to inaccurate and unreliable results. Here's a short description of why data leakage is a concern:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a39f2",
   "metadata": {},
   "source": [
    "#q53\n",
    "\n",
    "Target leakage and train-test contamination are two common issues that can affect the performance of machine learning models.\n",
    "\n",
    "Target leakage occurs when information that is not available at prediction time is used to train a model. This can lead to overfitting and poor generalization performance. For example, if we are trying to predict whether a customer will default on a loan, and we include information about whether they have already defaulted on a loan in the training data, this would be an example of target leakage.\n",
    "\n",
    "Train-test contamination occurs when information from the test set is used to train a model. This can lead to overfitting and poor generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f529cf",
   "metadata": {},
   "source": [
    "#q54\n",
    "\n",
    "some strategies are=\n",
    "\n",
    "'Understand the Problem Domain- Gain a deep understanding of the problem domain and the data involved. Clearly define the scope and context of the problem, including what information should and should not be available during model training and prediction.\n",
    "\n",
    "Data Exploration and Analysis- Thoroughly analyze the dataset to identify any potential sources of data leakage. Look for features that are highly correlated with the target variable or contain information that would not be available during real-world prediction.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8273ba4",
   "metadata": {},
   "source": [
    "#q55\n",
    "\n",
    "Some common source of data leakage are:\n",
    "    \n",
    "Weak and Stolen Credentials, a.k.a. Passwords.\n",
    "\n",
    "Back Doors, Application Vulnerabilities. \n",
    "\n",
    "Malware. \n",
    "\n",
    "Social Engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4250b5",
   "metadata": {},
   "source": [
    "#q56\n",
    "\n",
    "Cross-validation: If we perform feature selection or hyperparameter tuning on the entire dataset before performing cross-validation, this can lead to data leakage. Instead, we should perform feature selection or hyperparameter tuning on each fold of the cross-validation separately.\n",
    "\n",
    "Time-series data: If we use future information to predict past events, this would be an example of data leakage. For example, if we are trying to predict stock prices, and we use future stock prices to predict past stock prices, this would be an example of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca36d0",
   "metadata": {},
   "source": [
    "# Cross Validation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb720c3",
   "metadata": {},
   "source": [
    "#q57\n",
    "\n",
    " Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c73133",
   "metadata": {},
   "source": [
    "#q58\n",
    "\n",
    "Cross-validation is an important technique in machine learning for evaluating the performance and generalization capabilities of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a9e9c",
   "metadata": {},
   "source": [
    "#q59\n",
    "\n",
    "K-fold cross-validation and stratified k-fold cross-validation are two common techniques for evaluating machine learning models.\n",
    "\n",
    "K-fold cross-validation involves splitting the dataset into k equally sized folds, training the model on k-1 folds, and evaluating the model on the remaining fold. This process is repeated k times, with each fold being used as the test set exactly once. The results are then averaged over all k folds to obtain a final performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c8a21",
   "metadata": {},
   "source": [
    "#q60\n",
    "\n",
    "\n",
    "Cross-validation results can be interpreted by looking at the average performance across all folds. The most common performance metrics used for classification problems are accuracy, precision, recall, F1 score, and area under the ROC curve (AUC-ROC). For regression problems, common performance metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    "In addition to looking at the average performance across all folds, it is also important to look at the variance of the performance estimates. High variance may indicate that the model is overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e726424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
