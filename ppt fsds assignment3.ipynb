{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607d52a8",
   "metadata": {},
   "source": [
    "# General Linear Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f239354",
   "metadata": {},
   "source": [
    "#q1\n",
    "\n",
    "\n",
    "General Linear Model (GLM) is basically a statistical framework used to analyze relationships between variables. Its purpose is to provide a unified approach for examining the effects of multiple factors on an outcome variable, controlling for covariates, and testing hypotheses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0562acf",
   "metadata": {},
   "source": [
    "#q2\n",
    "\n",
    "\n",
    "The key assumptions for generalized linear models (GLMs) include a linear relationship between the independent and dependent variables, independence of residuals, and homoscedasticity of residuals. Violation of these assumptions may compromise the interpretation of model results and produce biased standard errors and unreliable p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6eaf0",
   "metadata": {},
   "source": [
    "#q3\n",
    "\n",
    " coefficient of a variable is interpreted as the change in the response based on a 1-unit change in the corresponding explanatory variable keeping all other variables held constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d78ee",
   "metadata": {},
   "source": [
    "\n",
    "#q4\n",
    "\n",
    "In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are tested independently, i.e., as multiple univariate tests with the same design matrix1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaeefc2",
   "metadata": {},
   "source": [
    "#q5\n",
    "\n",
    "interaction effects refer to the effect of one variable depending on the other variable. By default, interactions between predictor columns are expanded and computed on the fly as GLM iterates over dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a110fbc1",
   "metadata": {},
   "source": [
    "#q6\n",
    "\n",
    "\n",
    "categorical predictors can be handled by letting GLM handle categorical columns, as it can take advantage of the categorical column for better performance and memory utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440bbeb",
   "metadata": {},
   "source": [
    "#7\n",
    "\n",
    "Its purpose is to encode the relationship between the independent variables (predictors) and the dependent variable in a structured and mathematical form.\n",
    "Model specification,Calculation of predicted value,Assessment of model fit,Estimation of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ab963",
   "metadata": {},
   "source": [
    "#q8\n",
    "\n",
    "The most common approach is to use hypothesis testing, typically by conducting t-tests or F-tests.\n",
    "\n",
    "two common methods to test the significance of predictors in a GLM\n",
    "\n",
    "Individual Predictor Tests\n",
    "\n",
    "Overall Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b89cf",
   "metadata": {},
   "source": [
    "\n",
    "#q9\n",
    "\n",
    "Type I sums of squares are sequential and hierarchical. They test each term in the order they appear in the model.\n",
    "\n",
    "ype II sums of squares are similar to Type III, except that they preserve the principle of marginality. This means that main factors are tested in light of one another, but not in light of the interaction term.\n",
    "\n",
    "\n",
    "Type III sums of squares are “partial.” In essence, every term in the model is tested in light of every other term in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e91242",
   "metadata": {},
   "source": [
    "#q10\n",
    "\n",
    "GLM Deviance is a measure of error; lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case (saturated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0623ec1",
   "metadata": {},
   "source": [
    "# Regression:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592af8b",
   "metadata": {},
   "source": [
    "#q11\n",
    "\n",
    "Basically Regression analysis is a statistical method used to model and examine the relationship between a dependent variable and one or more independent variables. Its purpose is to understand how changes in the independent variables are associated with changes in the dependent variable and to make predictions or estimations based on this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62536b75",
   "metadata": {},
   "source": [
    "#q12\n",
    "\n",
    "Simple linear regression uses a single feature to model a linear relationship with a target variable. Multiple linear regression uses multiple features to model a linear relationship with a target variable. Multiple linear regression is more specific and complex than simple linear regression, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad83cd4",
   "metadata": {},
   "source": [
    "#q13\n",
    "\n",
    "R-squared value in regression represents the proportion of the variation in the dependent variable that can be explained by the independent variables. It provides a measure of how well the regression model fits the data. R-squared ranges from 0 to 1, where a higher value indicates a better fit. A value of 1 indicates that the model explains all the variation, while a value of 0 indicates that the model does not explain any variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce746a2",
   "metadata": {},
   "source": [
    "#q14\n",
    "\n",
    "Correlation and regression are two statistical methods used to study the relationship between two variables. Correlation measures the strength of the linear relationship between two variables, while regression is used to model the relationship between a dependent variable and one or more independent variables1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c6f15",
   "metadata": {},
   "source": [
    "#q15\n",
    "\n",
    "The coefficients in regression represent the change in Y for a one-unit change in X. In other words, they represent how much Y changes when X changes by one unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5682eb",
   "metadata": {},
   "source": [
    "#16\n",
    "\n",
    "common approaches to handle outliers in regression analysis\n",
    "Identify and remove outliers,\n",
    "\n",
    "Transform variables,\n",
    "\n",
    "Use robust regression methods,\n",
    "\n",
    "Consider non-parametric models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88726b6d",
   "metadata": {},
   "source": [
    "#q17\n",
    "\n",
    "Ridge regression tries to find the coefficients that minimize the mean squared error and wants the magnitude of coefficients to be as small as possible3.\n",
    "Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f966e5c",
   "metadata": {},
   "source": [
    "#q18\n",
    "\n",
    "Heteroscedasticity in regression refers to a situation where the variability of the residuals (or errors) of a regression model is not constant across all levels of the independent variables\n",
    "it can affect the models as\n",
    "\n",
    "Incorrect standard errors\n",
    "\n",
    "Inefficient estimates\n",
    "\n",
    "Incorrect confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953aaba",
   "metadata": {},
   "source": [
    "#q19\n",
    "\n",
    "Use techniques such as principal components analysis (PCA) or partial least squares regression (PLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b55b1",
   "metadata": {},
   "source": [
    "#q20\n",
    "\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial. In short, it involves fitting a polynomial equation to the data points to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80855ad",
   "metadata": {},
   "source": [
    "# Loss function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e57cc",
   "metadata": {},
   "source": [
    "#q21\n",
    "\n",
    "loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value)loss is a number indicating how bad the model's prediction was on a single example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082c728",
   "metadata": {},
   "source": [
    "#q22\n",
    "\n",
    "difference between a convex and non-convex loss function lies in their shape and properties.\n",
    "\n",
    "convex loss function is one that forms a convex curve when plotted against the model parameters. In a convex function, any line segment connecting two points on the curve lies entirely above or on the curve.\n",
    "\n",
    "non-convex loss function does not form a convex curve and may have multiple local minima and maxima. In non-convex functions, line segments connecting two points on the curve can lie below the curve, making it more challenging to find the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394985b",
   "metadata": {},
   "source": [
    "#q23\n",
    "\n",
    "Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss\n",
    "\n",
    "To find the MSE, take the observed value, subtract the predicted value, and square that difference. Repeat that for all observations. Then, sum all of those squared values and divide by the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e45a16",
   "metadata": {},
   "source": [
    "#q24\n",
    "\n",
    "MAE is calculated as the sum of absolute errors divided by the sample size: It is thus an arithmetic average of the absolute errors , where is the prediction and\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1933bdd",
   "metadata": {},
   "source": [
    "#q25\n",
    "\n",
    "Log loss, also known as cross-entropy loss or logistic loss, is a loss function commonly used in classification problems. It measures the performance of a model that predicts probabilities for multiple classes. The lower the log loss, the better the model's predictions align with the true class labels\n",
    "\n",
    "log_loss = - (1/N) * Σ(y * log(p) + (1 - y) * log(1 - p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e892b",
   "metadata": {},
   "source": [
    "#q26.\n",
    "\n",
    " various factors involved in choosing a loss function for specific problem such as type of machine learning algorithm chosen, ease of calculating the derivatives and to some degree the percentage of outliers in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358fa20",
   "metadata": {},
   "source": [
    "#q27\n",
    "\n",
    "In the context of loss functions, regularization introduces a penalty term that encourages the model to find a solution that is not only accurate on the training data but also simple and less prone to overfitting. The regularization term is typically based on the model's parameters and can take different forms, such as L1 regularization (Lasso), L2 regularization (Ridge), or a combination of both (Elastic Net)In the context of loss functions, regularization introduces a penalty term that encourages the model to find a solution that is not only accurate on the training data but also simple and less prone to overfitting. The regularization term is typically based on the model's parameters and can take different forms, such as L1 regularization (Lasso), L2 regularization (Ridge), or a combination of both (Elastic Net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626918c8",
   "metadata": {},
   "source": [
    "#q28\n",
    "\n",
    "Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss\n",
    "Huber loss function is used in robust regression to handle outliers. It is less sensitive to outliers than the squared error loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54429ae5",
   "metadata": {},
   "source": [
    "#q29\n",
    "\n",
    " it is a loss function used in quantile regression. It measures the deviation between the predicted quantiles of a model and the corresponding true quantiles of the target variable. Quantile regression focuses on estimating different quantiles of the conditional distribution of the response variable, offering a more comprehensive understanding of the relationship between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb39b5",
   "metadata": {},
   "source": [
    "#q30\n",
    "\n",
    "Squared loss (MSE) and absolute loss (MAE) are different ways of measuring the discrepancy between predicted and true values in regression tasks. Squared loss gives higher weight to larger errors, is differentiable, and sensitive to outliers. Absolute loss treats all errors equally, is not differentiable at zero, and is more robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d1ced",
   "metadata": {},
   "source": [
    "# Optimizer (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f6942",
   "metadata": {},
   "source": [
    "#Q31\n",
    "\n",
    "\n",
    "An optimizer is a  algorithm or function that adjusts the parameters of a model in order to minimize the error between the predicted output and the actual output\n",
    "The purpose of an optimizer is to find the optimal set of weights and biases that minimize the loss function of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089e175",
   "metadata": {},
   "source": [
    "#q32\n",
    "\n",
    "Gradient Descent (GD) is an iterative optimization algorithm used to minimize a given function, typically a loss function, by adjusting the parameters of a model. It is widely used in machine learning and deep learning for parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fc397",
   "metadata": {},
   "source": [
    "#q33\n",
    "\n",
    "key variations are:\n",
    "    \n",
    "    Batch Gradient Descent (BGD)=BGD computes the gradients of the entire training dataset to update the model parameters in each iteration\n",
    "    \n",
    "    Stochastic Gradient Descent (SGD): SGD updates the model parameters using the gradients computed for each individual training example\n",
    "        \n",
    "    Mini-Batch Gradient Descent: Mini-batch GD is a compromise between BGD and SGD   \n",
    "        \n",
    "     Momentum-based Gradient Descent: Momentum-based GD incorporates a momentum term accumulates past gradients to accelerate convergence\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a441493",
   "metadata": {},
   "source": [
    "#q34\n",
    "\n",
    "\n",
    "In gradient descent, the learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function\n",
    "There are several methods for choosing an appropriate learning rate. One common method is to use a fixed learning rate that is chosen based on prior knowledge or experimentation. Another method is to use an adaptive learning rate that changes over time based on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff90e2",
   "metadata": {},
   "source": [
    "#q35.\n",
    "\n",
    "Gradient Descent (GD) is a commonly used optimization algorithm in machine learning and can handle local optima in optimization problems through various approaches\n",
    "\n",
    "Initialization\n",
    "\n",
    "Learning Rate\n",
    "\n",
    "Stochasticity\n",
    "\n",
    "Momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b293ef",
   "metadata": {},
   "source": [
    "#q36\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm that is used for optimizing machine learning models. It addresses the computational inefficiency of traditional Gradient Descent methods when dealing with large datasets in machine learning projects1. SGD is an iterative method for optimizing an objective function that is smooth enough to be differentiable or subdifferentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0a8d2",
   "metadata": {},
   "source": [
    "#q37\n",
    "\n",
    "In Gradient Descent (GD), the batch size refers to the number of training examples used in each iteration to compute the gradient and update the model parameters\n",
    "\n",
    " choice of batch size can have a significant impact on the training process and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f43a4b",
   "metadata": {},
   "source": [
    "#q38\n",
    "\n",
    "Momentum is a strategy for accelerating the convergence of the optimization process by including a momentum element in the update rule. This momentum factor assists the optimizer in continuing to go in the same direction even if the gradient changes direction or becomes zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a5c00",
   "metadata": {},
   "source": [
    "#q39\n",
    "\n",
    "Batch Gradient Descent (BGD) computes the gradient of the cost function w.r.t. to the parameters θ for the entire training dataset. It is slow and computationally expensive for large datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab1a11",
   "metadata": {},
   "source": [
    "#q40\n",
    "\n",
    "In Gradient Descent (GD), the batch size refers to the number of training examples used in each iteration to compute the gradient and update the model parameters.\n",
    "\n",
    "Batch Gradient Descent (BGD): In BGD, the batch size is set to the total number of training examples available\n",
    "    \n",
    " Stochastic Gradient Descent (SGD): In SGD, the batch size is set to 1, meaning that only one training example is used to compute the gradient and update the parameters in each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c148cb5",
   "metadata": {},
   "source": [
    "# Regularization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e31230",
   "metadata": {},
   "source": [
    "#q41\n",
    "\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7935510",
   "metadata": {},
   "source": [
    "#q42\n",
    "\n",
    "L1 and L2 regularization are techniques used to prevent overfitting in machine learning by adding a penalty term to the cost function. L1 regularization uses the absolute value of the weights, while L2 regularization uses the squared value of the weights. L1 regularization tends to estimate the median of the data, while L2 regularization tends to estimate the mean of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb62fdd",
   "metadata": {},
   "source": [
    "#q43\n",
    "\n",
    "Ridge regression is a variant of linear regression that incorporates regularization to handle multicollinearity (high correlation) among predictor variables and prevent overfitting. It introduces a regularization term, also known as the L2 penalty, to the standard linear regression objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65dd3b",
   "metadata": {},
   "source": [
    "#q44\n",
    "\n",
    "Elastic Net Regularization is a regularization technique that uses both L1 and L2 regularizations to produce most optimized output\n",
    "\n",
    "the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7b38e",
   "metadata": {},
   "source": [
    "#q45\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the cost function. The penalty term is added to the cost function to reduce the complexity of the model and the influence of noise on the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694f5ab",
   "metadata": {},
   "source": [
    "#q46\n",
    "\n",
    "Early stopping is a technique used in machine learning to prevent overfitting and improve model generalization by monitoring the performance of the model during training and stopping the training process before it fully converges. It relates to regularization in the sense that it acts as a form of regularization that helps avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe3f75",
   "metadata": {},
   "source": [
    "#q47\n",
    "\n",
    "Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a6921",
   "metadata": {},
   "source": [
    "#q48\n",
    "\n",
    "regularization parameter is a hyperparameter that controls the amount of regularization applied to a model. It is used to prevent overfitting by adding a penalty term to the loss function of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffe1d1",
   "metadata": {},
   "source": [
    "#q49\n",
    "\n",
    "Early stopping is a technique used in machine learning to prevent overfitting and improve model generalization by monitoring the performance of the model during training and stopping the training process before it fully converges\n",
    "\n",
    "In early stopping, the training process is monitored using a validation set or a separate subset of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874b450",
   "metadata": {},
   "source": [
    "#q50\n",
    "\n",
    "Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa9e60",
   "metadata": {},
   "source": [
    "# SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8ce73",
   "metadata": {},
   "source": [
    "#q51\n",
    "\n",
    "\n",
    "Support Vector Machines (SVM) is a supervised learning algorithm that can be used for both classification and regression tasks. The algorithm is trained on a dataset of labeled examples, where each example is a vector in n-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc352c",
   "metadata": {},
   "source": [
    "#q52\n",
    "\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map the input data into a higher-dimensional feature space without explicitly computing the transformed feature vectors. It enables SVMs to efficiently handle non-linearly separable data by operating in a higher-dimensional space without the need to explicitly compute the transformed feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43e587",
   "metadata": {},
   "source": [
    "#q53\n",
    "\n",
    "SVM algorithm finds the closest point of the lines from both the classes. These points are called support vectors. The distance between the vectors and the hyperplane is called as margin. And the goal of SVM is to maximize this margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51a663",
   "metadata": {},
   "source": [
    "#q54\n",
    "\n",
    "The margin in SVM is the distance between the hyperplane and the closest data points from each class. The margin is maximized during training to ensure that the model generalizes well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a1353",
   "metadata": {},
   "source": [
    "#q55\n",
    "\n",
    "Handling unbalanced datasets in SVM can be important to ensure that the model effectively learns from both classes, especially when one class has significantly more samples than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc876adb",
   "metadata": {},
   "source": [
    "#q56\n",
    "\n",
    "When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a496af",
   "metadata": {},
   "source": [
    "#q57\n",
    "\n",
    "The C-parameter in SVM is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error on the training data. A smaller value of C will result in a larger margin but more misclassifications on the training data. A larger value of C will result in a smaller margin but fewer misclassifications on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe63193",
   "metadata": {},
   "source": [
    "#q58\n",
    "\n",
    "Slack variables are an important concept in Support Vector Machines (SVMs) that allow for the classification of non-linearly separable data or data with overlapping classes. They introduce a soft margin to SVMs by allowing some data points to be misclassified or fall within the margin boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114b9be",
   "metadata": {},
   "source": [
    "#q59\n",
    "\n",
    "The difference between a hard margin and a soft margin in SVMs lies in the separability of the data. If our data is linearly separable, we go for a hard margin. However, if this is not the case, it won't be feasible to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ac3d0",
   "metadata": {},
   "source": [
    "#q60\n",
    "\n",
    "In a linear SVM model, the coefficients represent the weights assigned to each feature in the dataset. The coefficients can be accessed using the coef_ attribute of the trained SVM model. The sign of the coefficient indicates whether the feature has a positive or negative impact on the classification decision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e09a9b",
   "metadata": {},
   "source": [
    "# Decision Trees:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4795a",
   "metadata": {},
   "source": [
    "#q61\n",
    "\n",
    "A decision tree algorithm is a machine learning algorithm that uses a decision tree to make predictions. It follows a tree-like model of decisions and their possible consequences. The algorithm works by recursively splitting the data into subsets based on the most significant feature at each node of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d18b73",
   "metadata": {},
   "source": [
    "#q62\n",
    "\n",
    "n a decision tree, the process of making splits is crucial for dividing the data based on different features to create branches and eventually form a tree-like structure. The goal is to find the most informative features and split points that maximize the separation of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc738e",
   "metadata": {},
   "source": [
    "#q63\n",
    "\n",
    "mpurity measures are used in Decision Trees to arrive at as lowest impurity as possible. Impurity is the presence of more than one class in a subset of data. There are several indices to measure degree of impurity quantitatively, including: 1) Entropy 2) Gini index 3) Classification error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72486021",
   "metadata": {},
   "source": [
    "#q64\n",
    "\n",
    "Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868df79f",
   "metadata": {},
   "source": [
    "#q65\n",
    "\n",
    "Handling missing values in decision trees depends on the specific algorithm or implementation used.\n",
    "common approach\n",
    "\n",
    "Ignore Missing Values\n",
    "\n",
    "Missing as a Separate Category:\n",
    "    \n",
    "Proportional Splitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107934c2",
   "metadata": {},
   "source": [
    "#q66\n",
    "\n",
    "Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cb263",
   "metadata": {},
   "source": [
    "#q67\n",
    "\n",
    "Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5b06a",
   "metadata": {},
   "source": [
    "#q68\n",
    "\n",
    "Interpreting decision boundaries in a decision tree involves understanding how the tree's structure and splitting criteria determine the regions in the feature space where different predictions or classifications are made\n",
    "\n",
    "general appraoch\n",
    "\n",
    "Tree Structure\n",
    "\n",
    "Feature Space\n",
    "\n",
    "Visualization\n",
    "\n",
    "Interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79d6c1",
   "metadata": {},
   "source": [
    "#q69\n",
    "\n",
    "Feature importance is a technique used to determine which features in a dataset are most important in predicting the target variable\n",
    "The feature importance can be used to identify which features are most important for prediction and can be used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0d396",
   "metadata": {},
   "source": [
    "#q70\n",
    "\n",
    "Ensemble techniques in machine learning involve combining multiple individual models to make more accurate predictions or classifications than what each individual model could achieve alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c31ec",
   "metadata": {},
   "source": [
    "# Ensemble Techniques:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6423c45",
   "metadata": {},
   "source": [
    "#q71\n",
    "\n",
    "Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods in machine learning usually produce more accurate solutions than a single model would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d22b9",
   "metadata": {},
   "source": [
    "#q72\n",
    "\n",
    "What is Bagging in Machine Learning And How to Perform Bagging\n",
    "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4027f4",
   "metadata": {},
   "source": [
    "#q73\n",
    "\n",
    "Bootstrapping is a statistical technique that involves resampling a dataset with replacement to create new datasets that are used to estimate the population parameters. In bagging, bootstrapping is used to create multiple training datasets from the original dataset by randomly sampling with replacement from the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad887e7",
   "metadata": {},
   "source": [
    "#q74\n",
    "\n",
    "Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551df729",
   "metadata": {},
   "source": [
    "#q75\n",
    "\n",
    "AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost.18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b54808",
   "metadata": {},
   "source": [
    "#q76\n",
    "\n",
    "\n",
    "The purpose of Random Forests in ensemble learning is to improve prediction accuracy, reduce overfitting, and handle high-dimensional data by combining multiple decision trees. Random Forests is a powerful ensemble technique that leverages the concept of bagging (Bootstrap Aggregating) and introduces additional randomness through feature sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e9813",
   "metadata": {},
   "source": [
    "#q77\n",
    "\n",
    "Random Forests provide a measure of feature importance by evaluating the contribution of each feature in the ensemble of decision trees. The importance of a feature is calculated based on the reduction in impurity or information gain resulting from splits using that feature across all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf021d4e",
   "metadata": {},
   "source": [
    "#q78\n",
    "\n",
    "Stacking, also known as stacked generalization, is an ensemble learning technique that combines predictions from multiple models, including different algorithms or variations, by training a meta-model that learns how to best combine their predictions. Stacking goes beyond simple averaging or voting and aims to leverage the strengths of different models to improve predictive performance. Here's how stacking work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ec890",
   "metadata": {},
   "source": [
    "#q79\n",
    "\n",
    "Advantages of Ensemble Techniques:\n",
    "    \n",
    "    improved Predictive Performanc\n",
    "    \n",
    "    Reduced Overfitting\n",
    "    \n",
    "    Robustness\n",
    "    \n",
    "    Tuning and Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34448682",
   "metadata": {},
   "source": [
    "#q80\n",
    "\n",
    "Choosing the optimal number of models in an ensemble depends on various factors, including the dataset, the complexity of the problem, and the computational resources available\n",
    "\n",
    "some optimal number of models in an ensemble\n",
    "\n",
    "Empirical Evaluation\n",
    "\n",
    "Computational Resources\n",
    "\n",
    "Bias-Variance Trade-Of\n",
    "\n",
    "Ensemble Size Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309ba36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
